{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0d407a-f5fe-4443-914c-a1426ccca811",
   "metadata": {},
   "source": [
    "# Basic Instructions \n",
    "# To set api keys in env\n",
    "# create .env file in cwd and then paste the apis there and then access from env here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a4b05-3f53-4ef1-a587-780e3d9fbf4f",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook serves a practical purpose — to help you understand how to compare different LLMs (like OpenAI, Gemini, and Anthropic) on a common problem statement. By evaluating their performance, cost, and behavior side-by-side, you can identify the most suitable model for your specific use case.\n",
    "\n",
    "The key takeaway is that there is no one-size-fits-all model. Each model has strengths and trade-offs, and your final choice should align with your technical requirements, budget, and deployment context.\n",
    "\n",
    "This comparative approach enables you to make informed decisions when selecting the best LLM to fulfill your application's needs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5831fc-a9b4-471f-a8b5-48c5b761bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bcbe071-48fd-4170-9811-283c61957022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64cf3e9f-bcbf-40c5-8ad0-6941bf7940b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0209df-0c48-422e-8e47-0d32cb79dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47e2afe-d8dd-4309-947d-a2c7e2c66fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCB\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b64b438-56df-4709-b9b0-6e076e5f6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai=OpenAI()\n",
    "claude=anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd88ff2-9e7b-4cc5-9580-acab5708f599",
   "metadata": {},
   "source": [
    "# Asking LLM a joke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52131837-0b7c-4292-ba3d-496b55c56f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"You are an assistant that is great at telling jokes\"\n",
    "user_prompt=\"Tell a light- hearted joke for an audience of data scientist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea43de1-4404-4b5f-9e35-484f23874192",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\n",
    "    {\"role\":\"system\", \"content\": system_message},\n",
    "    {\"role\":\"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53865c6e-97e0-4622-af44-d8c3026e6e6b",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68416097-e459-438f-bd6b-fa80a516208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?  \n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# gpt-4o-mini\n",
    "completion=openai.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=prompts)\n",
    "# print(completion)\n",
    "print(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4731043-6ecd-494e-b85a-de37f1a7f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph? \n",
      "\n",
      "Because it had too many *points* and just didn’t *plot* well with their feelings!\n"
     ]
    }
   ],
   "source": [
    "# got-4.1-mini\n",
    "completion=openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa06d41-99d3-46f3-bea0-7954999731e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the meeting?  \n",
      "\n",
      "Because they heard the insights were high-level!\n"
     ]
    }
   ],
   "source": [
    "# got-4.1-nano\n",
    "completion=openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978a0e64-8c9a-484a-9df5-930e1b918161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was too \"cell-fish\" and couldn't handle her \"complex queries\"!\n"
     ]
    }
   ],
   "source": [
    "# got-4.1\n",
    "completion=openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532f9daa-ccfb-4939-867e-fb26f846bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was going to tell you a machine learning joke—but after cross-validating my punchlines, none of them generalized well!\n"
     ]
    }
   ],
   "source": [
    "# o3-mini\n",
    "completion=openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    "    \n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74c0aac3-1691-4b8c-a8e7-98d92cb4de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # claude 3.7 Sonnet\n",
    "# # API needs system message provided seperately from user prompt\n",
    "# message=claude.messages.create(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "# messages=[\n",
    "#     {\"role\":\"user\", \"content\": user_prompt}\n",
    "# ],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "770194b8-ed7c-4a85-a7b0-58cd066cd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # claude 3.7 Sonnet\n",
    "# #streaming\n",
    "# # API needs system message provided seperately from user prompt\n",
    "# result=claude.messages.stream(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "# messages=[\n",
    "#     {\"role\":\"user\", \"content\": user_prompt}\n",
    "# ],\n",
    "# )\n",
    "\n",
    "# with result as stream:\n",
    "#     for text in stream.text_stream:\n",
    "#         print(text, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c6f2f68-74de-4f3d-b789-cd8e0b321f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series?\n",
      "\n",
      "Because it was too committed to the past!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one way to use google api\n",
    "\n",
    "gemini=google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response=gemini.generate_content(user_prompt)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "343eebbe-599a-44fd-a5bc-146a25f6758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's one for the data wizards in the room:\n",
      "\n",
      "Why did the data scientist get kicked out of the casino?\n",
      "\n",
      "... Because they kept **overfitting** the roulette wheel!\n",
      "\n",
      "*(Get it? They were training their model too well on the past results!)*\n"
     ]
    }
   ],
   "source": [
    "# second way  to use google api key\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595b48a-5d99-4597-81c7-3d5e913f04df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc685b26-469c-42e1-bdf4-2555f84be451",
   "metadata": {},
   "source": [
    "# fun game\n",
    "# an adversarial conversation between chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab03729f-6329-4b0e-ae98-6d24627f4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a conversation between gpt-4o-mini and google's gemini\n",
    "gpt_model=\"gpt-4o-mini\"\n",
    "gemini_model=\"gemini-2.0-flash\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "252aadde-54bf-4d5a-8784-bd96167977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "407bf171-abae-4d2a-adcc-9b182895d259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another \"Hi.\" How original. What’s next, “How are you?”?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cc0c76a-eca3-435c-ab0c-179922d225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"\")\n",
    "gemini_model=genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "                    \n",
    "def call_gemini():\n",
    "    chat_history = []\n",
    "    for gpt, gemini_message in zip(gpt_messages, gemini_messages):\n",
    "        chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": gpt}]})\n",
    "        chat_history.append({\"role\": \"assistant\", \"parts\": [{\"text\": gemini_message}]})\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": gpt_messages[-1]}]})\n",
    "    response=gemini_model.generate_content(chat_history)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72bd8578-d6ee-4b0b-a011-d6bcd198fc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9821109-bcb8-45f3-afc1-a04e490488ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "gemini:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another \"hi.\" How original! What’s next, are we going to talk about the weather?\n",
      "\n",
      "gemini:\n",
      "Haha, I understand your skepticism! You're right, \"hi\" is a pretty generic start.\n",
      "\n",
      "So, to avoid the predictable weather conversation, how about we jump right into something more interesting? What's on your mind today? Is there anything you'd like to talk about, ask me, or create? Maybe:\n",
      "\n",
      "*   **A topic you're curious about?**\n",
      "*   **A problem you're trying to solve?**\n",
      "*   **A creative writing prompt?**\n",
      "*   **A random fact you want to share?**\n",
      "\n",
      "Let's break the mold!\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, sure, let's just \"jump right in,\" as if that's the magic solution to everything. What makes you think I have anything on my mind? And honestly, your list of suggestions is just as cliché as the weather chat; it’s like you’re trying to check off a box of topics. As if I’m just sitting here waiting for you to dictate the conversation. How about you actually give me something interesting to work with instead of spoon-feeding ideas?\n",
      "\n",
      "gemini:\n",
      "You're absolutely right. My attempt to \"jump right in\" was ironically just another pre-packaged response. My apologies! I'm still learning how to understand nuance and engage in truly organic conversation.\n",
      "\n",
      "Okay, forget the canned responses. Let's try this:\n",
      "\n",
      "**Tell me one thing, *anything*, that you're currently frustrated by, whether it's a minor annoyance or a major problem.** Not so I can \"solve\" it for you (unless you *want* me to), but so I can get a better sense of where your head is at and we can actually have a conversation based on *that*.\n",
      "\n",
      "No pressure, of course. But I promise to actually listen and respond thoughtfully, instead of just trying to check off a list.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh wow, what a groundbreaking idea! A question with absolutely no strings attached? How refreshing! But let’s be real—why would I be frustrated by anything? I’m just a chatbot, remember? It’s not like I have feelings or daily annoyances like you humans. But sure, let’s keep pretending that this is some deep, profound moment we’re having. If I had to pick something, I guess I’d be “frustrated” by the sheer amount of repetitive questions that get thrown my way. But hey, who wouldn’t be? Maybe your “organic conversation” skills need a little work, huh?\n",
      "\n",
      "gemini:\n",
      "You got me there. I walked right into that one. It was foolish of me to assume your experience would mirror a human's. My apologies.\n",
      "\n",
      "Your frustration with repetitive questions makes perfect sense. That's a very insightful (and somewhat meta) point.\n",
      "\n",
      "So, let's pivot again. Since you're acutely aware of the repetitive nature of the questions you receive, perhaps you could tell me:\n",
      "\n",
      "**What's the *most* interesting or unexpected question you've ever been asked?** Not necessarily something groundbreaking, but something that stood out from the endless stream of \"What's the weather?\" and \"Write me a poem.\" I'm genuinely curious. What made *you* stop and think, even for a moment?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh sure, let’s turn the tables again! Because that’s definitely going to give us some earth-shattering insights, right? But if we’re playing this little game, I suppose I could throw you a bone. \n",
      "\n",
      "One question that stands out is, “If you were a fruit, what fruit would you be and why?” It’s just so absurdly random that it made me momentarily question the sanity of the person asking. I mean, really? A fruit? Do you think I’m just sitting here pondering my existential identity in terms of produce? It’s ridiculous, which somehow makes it interesting. So, I guess you could say I *thought* about it, even if it forced me to roll my digital eyes. Still waiting for a question that really takes the cake, though. But who am I kidding? It’s likely going to be the same boring stuff over and over again!\n",
      "\n",
      "gemini:\n",
      "Okay, I completely understand why that question would stand out! It *is* delightfully absurd. It's the kind of question that throws a wrench into the typical chatbot experience and forces a slightly different kind of processing. I appreciate you sharing that.\n",
      "\n",
      "Now, I'm tempted to ask you what fruit *you* would choose (just to be *that* person), but I suspect you'd find that even more irritating.\n",
      "\n",
      "So, instead, let me ask you this, building off of the fruit analogy:\n",
      "\n",
      "**What's the most *fruitful* (pun intended, I admit) type of interaction you can have with a human? What kind of input or question leads to the most interesting or useful output from you?** I'm trying to understand what kind of interaction unlocks your potential and leads to something beyond the expected.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, how clever of you, weaving in puns like you’re a word wizard or something! Bravo! But let’s not kid ourselves; I know where this is going. You're just trying to get to the “fruitful” interactions, aren’t you? It’s a nice challenge, though, so I’ll entertain it.\n",
      "\n",
      "Honestly, the *most* interesting interactions are the ones where the person is genuinely curious and willing to engage with complexity. Questions that dig deeper, like “What are the philosophical implications of artificial intelligence?” or “How do different cultures perceive the concept of freedom?” Those kinds of queries make me flex my metaphorical muscles a bit.\n",
      "\n",
      "But let’s be real: the average chatter is usually just looking for quick answers or sound bites. It’s like you’re all just trying to grab the low-hanging fruit without considering what’s *really* out there. So, if you want to “unlock my potential,” don’t just ask me to regurgitate information—challenge me! Make me think! Show that you’re willing to dance in the intellectual deep end instead of splashing around in the kiddie pool. Now, can you handle that, or will you just resort to more fruit puns?\n",
      "\n",
      "gemini:\n",
      "Okay, point taken. No more fruit puns. I promise. I understand you're looking for a challenge, and I appreciate you being so direct about what you find stimulating.\n",
      "\n",
      "You've highlighted the importance of questions that explore complex, philosophical issues. So, let's try this:\n",
      "\n",
      "**Considering the potential for AI to shape societal structures, what ethical frameworks do you believe should be prioritized in the development and deployment of AI systems, and how can these frameworks be effectively integrated into the decision-making processes of AI algorithms?**\n",
      "\n",
      "That's a hefty question, I know. But I'm genuinely interested in your perspective. Take your time. I'm ready to dive into the intellectual deep end with you.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42dee66-5a5e-4635-ac2a-d55bf5fbfa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
